{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mariamfarghali/DeepLearning_projects/blob/main/main/_downloads/63ad2005fc24f143f3f078cd2c6b0d60/tacotron2_pipeline_tutorial.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "po-s_JPhlExo"
      },
      "source": [
        "\n",
        "# Text-to-Speech with Tacotron2\n",
        "\n",
        "**Author**: [Yao-Yuan Yang](https://github.com/yangarbiter)_,\n",
        "[Moto Hira](moto@meta.com)_\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-sDPztWclExu"
      },
      "source": [
        "## Overview\n",
        "\n",
        "This tutorial shows how to build text-to-speech pipeline, using the\n",
        "pretrained Tacotron2 in torchaudio.\n",
        "\n",
        "The text-to-speech pipeline goes as follows:\n",
        "\n",
        "1. Text preprocessing\n",
        "\n",
        "   First, the input text is encoded into a list of symbols. In this\n",
        "   tutorial, we will use English characters and phonemes as the symbols.\n",
        "\n",
        "2. Spectrogram generation\n",
        "\n",
        "   From the encoded text, a spectrogram is generated. We use the ``Tacotron2``\n",
        "   model for this.\n",
        "\n",
        "3. Time-domain conversion\n",
        "\n",
        "   The last step is converting the spectrogram into the waveform. The\n",
        "   process to generate speech from spectrogram is also called a Vocoder.\n",
        "   In this tutorial, three different vocoders are used,\n",
        "   :py:class:`~torchaudio.models.WaveRNN`,\n",
        "   :py:class:`~torchaudio.transforms.GriffinLim`, and\n",
        "   [Nvidia's WaveGlow](https://pytorch.org/hub/nvidia_deeplearningexamples_tacotron2/)_.\n",
        "\n",
        "\n",
        "The following figure illustrates the whole process.\n",
        "\n",
        "<img src=\"https://download.pytorch.org/torchaudio/tutorial-assets/tacotron2_tts_pipeline.png\">\n",
        "\n",
        "All the related components are bundled in :py:class:`torchaudio.pipelines.Tacotron2TTSBundle`,\n",
        "but this tutorial will also cover the process under the hood.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sBXPGwyXlExw"
      },
      "source": [
        "## Preparation\n",
        "\n",
        "First, we install the necessary dependencies. In addition to\n",
        "``torchaudio``, ``DeepPhonemizer`` is required to perform phoneme-based\n",
        "encoding.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "vumJc9iolExx",
        "outputId": "6937fb95-34b5-4431-cf56-1b98e09fdf35",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting deep_phonemizer\n",
            "  Downloading deep-phonemizer-0.0.19.tar.gz (29 kB)\n",
            "  Preparing metadata (setup.py): started\n",
            "  Preparing metadata (setup.py): finished with status 'done'\n",
            "Requirement already satisfied: torch>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from deep_phonemizer) (2.5.1+cu124)\n",
            "Requirement already satisfied: tqdm>=4.38.0 in /usr/local/lib/python3.11/dist-packages (from deep_phonemizer) (4.67.1)\n",
            "Requirement already satisfied: PyYAML>=5.1 in /usr/local/lib/python3.11/dist-packages (from deep_phonemizer) (6.0.2)\n",
            "Requirement already satisfied: tensorboard in /usr/local/lib/python3.11/dist-packages (from deep_phonemizer) (2.18.0)\n",
            "Requirement already satisfied: certifi>=2022.12.7 in /usr/local/lib/python3.11/dist-packages (from deep_phonemizer) (2025.1.31)\n",
            "Requirement already satisfied: wheel>=0.38.0 in /usr/local/lib/python3.11/dist-packages (from deep_phonemizer) (0.45.1)\n",
            "Requirement already satisfied: setuptools>=65.5.1 in /usr/local/lib/python3.11/dist-packages (from deep_phonemizer) (75.1.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=1.2.0->deep_phonemizer) (3.17.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.2.0->deep_phonemizer) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.2.0->deep_phonemizer) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.2.0->deep_phonemizer) (3.1.5)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=1.2.0->deep_phonemizer) (2024.10.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch>=1.2.0->deep_phonemizer)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch>=1.2.0->deep_phonemizer)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch>=1.2.0->deep_phonemizer)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=1.2.0->deep_phonemizer)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch>=1.2.0->deep_phonemizer)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch>=1.2.0->deep_phonemizer)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch>=1.2.0->deep_phonemizer)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=1.2.0->deep_phonemizer)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=1.2.0->deep_phonemizer)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.2.0->deep_phonemizer) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.2.0->deep_phonemizer) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=1.2.0->deep_phonemizer)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.2.0->deep_phonemizer) (3.1.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.2.0->deep_phonemizer) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.2.0->deep_phonemizer) (1.3.0)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.11/dist-packages (from tensorboard->deep_phonemizer) (1.4.0)\n",
            "Requirement already satisfied: grpcio>=1.48.2 in /usr/local/lib/python3.11/dist-packages (from tensorboard->deep_phonemizer) (1.70.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard->deep_phonemizer) (3.7)\n",
            "Requirement already satisfied: numpy>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard->deep_phonemizer) (1.26.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from tensorboard->deep_phonemizer) (24.2)\n",
            "Requirement already satisfied: protobuf!=4.24.0,>=3.19.6 in /usr/local/lib/python3.11/dist-packages (from tensorboard->deep_phonemizer) (4.25.6)\n",
            "Requirement already satisfied: six>1.9 in /usr/local/lib/python3.11/dist-packages (from tensorboard->deep_phonemizer) (1.17.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard->deep_phonemizer) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard->deep_phonemizer) (3.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard->deep_phonemizer) (3.0.2)\n",
            "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 363.4/363.4 MB 4.0 MB/s eta 0:00:00\n",
            "Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 13.8/13.8 MB 74.9 MB/s eta 0:00:00\n",
            "Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 24.6/24.6 MB 59.2 MB/s eta 0:00:00\n",
            "Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 883.7/883.7 kB 49.3 MB/s eta 0:00:00\n",
            "Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 664.8/664.8 MB 2.9 MB/s eta 0:00:00\n",
            "Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 211.5/211.5 MB 5.5 MB/s eta 0:00:00\n",
            "Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 56.3/56.3 MB 9.8 MB/s eta 0:00:00\n",
            "Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 127.9/127.9 MB 7.5 MB/s eta 0:00:00\n",
            "Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 207.5/207.5 MB 6.6 MB/s eta 0:00:00\n",
            "Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 21.1/21.1 MB 59.5 MB/s eta 0:00:00\n",
            "Building wheels for collected packages: deep_phonemizer\n",
            "  Building wheel for deep_phonemizer (setup.py): started\n",
            "  Building wheel for deep_phonemizer (setup.py): finished with status 'done'\n",
            "  Created wheel for deep_phonemizer: filename=deep_phonemizer-0.0.19-py3-none-any.whl size=33273 sha256=53399d4de0bc31d5182f3f92278fc2597c52171bbc7ebec6fa44eee049f315d7\n",
            "  Stored in directory: /root/.cache/pip/wheels/73/cc/01/1c74a1f4e6ba31a42bb82f4e3d852e2f23236fe3e5d589dcf3\n",
            "Successfully built deep_phonemizer\n",
            "Installing collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, deep_phonemizer\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed deep_phonemizer-0.0.19 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\n"
          ]
        }
      ],
      "source": [
        "%%bash\n",
        "pip3 install deep_phonemizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "-7BaZi_elExz",
        "outputId": "86a600be-6ceb-4690-ab91-1f071415f05b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.5.1+cu124\n",
            "2.5.1+cu124\n",
            "cuda\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torchaudio\n",
        "\n",
        "torch.random.manual_seed(0)\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "print(torch.__version__)\n",
        "print(torchaudio.__version__)\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "58t-MqTflEx0"
      },
      "outputs": [],
      "source": [
        "import IPython\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wLg9EUjwlEx1"
      },
      "source": [
        "## Text Processing\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M1--3wsLlEx1"
      },
      "source": [
        "### Character-based encoding\n",
        "\n",
        "In this section, we will go through how the character-based encoding\n",
        "works.\n",
        "\n",
        "Since the pre-trained Tacotron2 model expects specific set of symbol\n",
        "tables, the same functionalities is available in ``torchaudio``. However,\n",
        "we will first manually implement the encoding to aid in understanding.\n",
        "\n",
        "First, we define the set of symbols\n",
        "``'_-!\\'(),.:;? abcdefghijklmnopqrstuvwxyz'``. Then, we will map the\n",
        "each character of the input text into the index of the corresponding\n",
        "symbol in the table. Symbols that are not in the table are ignored.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0m4z4Ve9lEx2"
      },
      "outputs": [],
      "source": [
        "symbols = \"_-!'(),.:;? abcdefghijklmnopqrstuvwxyz\"\n",
        "look_up = {s: i for i, s in enumerate(symbols)}\n",
        "symbols = set(symbols)\n",
        "\n",
        "\n",
        "def text_to_sequence(text):\n",
        "    text = text.lower()\n",
        "    return [look_up[s] for s in text if s in symbols]\n",
        "\n",
        "\n",
        "text = \"Hello world! Text to speech!\"\n",
        "print(text_to_sequence(text))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eNn5yv60lEx3"
      },
      "source": [
        "As mentioned in the above, the symbol table and indices must match\n",
        "what the pretrained Tacotron2 model expects. ``torchaudio`` provides the same\n",
        "transform along with the pretrained model. You can\n",
        "instantiate and use such transform as follow.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EfUWfjaclEx4"
      },
      "outputs": [],
      "source": [
        "processor = torchaudio.pipelines.TACOTRON2_WAVERNN_CHAR_LJSPEECH.get_text_processor()\n",
        "\n",
        "text = \"Hello world! Text to speech!\"\n",
        "processed, lengths = processor(text)\n",
        "\n",
        "print(processed)\n",
        "print(lengths)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FOltenKPlEx4"
      },
      "source": [
        "Note: The output of our manual encoding and the ``torchaudio`` ``text_processor`` output matches (meaning we correctly re-implemented what the library does internally). It takes either a text or list of texts as inputs.\n",
        "When a list of texts are provided, the returned ``lengths`` variable\n",
        "represents the valid length of each processed tokens in the output\n",
        "batch.\n",
        "\n",
        "The intermediate representation can be retrieved as follows:\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LIFJlA8flEx6"
      },
      "outputs": [],
      "source": [
        "print([processor.tokens[i] for i in processed[0, : lengths[0]]])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "on9JoWCTlEx6"
      },
      "source": [
        "### Phoneme-based encoding\n",
        "\n",
        "Phoneme-based encoding is similar to character-based encoding, but it\n",
        "uses a symbol table based on phonemes and a G2P (Grapheme-to-Phoneme)\n",
        "model.\n",
        "\n",
        "The detail of the G2P model is out of the scope of this tutorial, we will\n",
        "just look at what the conversion looks like.\n",
        "\n",
        "Similar to the case of character-based encoding, the encoding process is\n",
        "expected to match what a pretrained Tacotron2 model is trained on.\n",
        "``torchaudio`` has an interface to create the process.\n",
        "\n",
        "The following code illustrates how to make and use the process. Behind\n",
        "the scene, a G2P model is created using ``DeepPhonemizer`` package, and\n",
        "the pretrained weights published by the author of ``DeepPhonemizer`` is\n",
        "fetched.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GjaU7TP0lEx7"
      },
      "outputs": [],
      "source": [
        "bundle = torchaudio.pipelines.TACOTRON2_WAVERNN_PHONE_LJSPEECH\n",
        "\n",
        "processor = bundle.get_text_processor()\n",
        "\n",
        "text = \"Hello world! Text to speech!\"\n",
        "with torch.inference_mode():\n",
        "    processed, lengths = processor(text)\n",
        "\n",
        "print(processed)\n",
        "print(lengths)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zR4yESI6lEx8"
      },
      "source": [
        "Notice that the encoded values are different from the example of\n",
        "character-based encoding.\n",
        "\n",
        "The intermediate representation looks like the following.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h7o1ik3_lEx8"
      },
      "outputs": [],
      "source": [
        "print([processor.tokens[i] for i in processed[0, : lengths[0]]])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7_kQErg5lEx8"
      },
      "source": [
        "## Spectrogram Generation\n",
        "\n",
        "``Tacotron2`` is the model we use to generate spectrogram from the\n",
        "encoded text. For the detail of the model, please refer to [the\n",
        "paper](https://arxiv.org/abs/1712.05884)_.\n",
        "\n",
        "It is easy to instantiate a Tacotron2 model with pretrained weights,\n",
        "however, note that the input to Tacotron2 models need to be processed\n",
        "by the matching text processor.\n",
        "\n",
        ":py:class:`torchaudio.pipelines.Tacotron2TTSBundle` bundles the matching\n",
        "models and processors together so that it is easy to create the pipeline.\n",
        "\n",
        "For the available bundles, and its usage, please refer to\n",
        ":py:class:`~torchaudio.pipelines.Tacotron2TTSBundle`.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KILDIaQglEx9"
      },
      "outputs": [],
      "source": [
        "bundle = torchaudio.pipelines.TACOTRON2_WAVERNN_PHONE_LJSPEECH\n",
        "processor = bundle.get_text_processor()\n",
        "tacotron2 = bundle.get_tacotron2().to(device)\n",
        "\n",
        "text = \"Hello world! Text to speech!\"\n",
        "\n",
        "with torch.inference_mode():\n",
        "    processed, lengths = processor(text)\n",
        "    processed = processed.to(device)\n",
        "    lengths = lengths.to(device)\n",
        "    spec, _, _ = tacotron2.infer(processed, lengths)\n",
        "\n",
        "\n",
        "_ = plt.imshow(spec[0].cpu().detach(), origin=\"lower\", aspect=\"auto\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PYNENM0klEx-"
      },
      "source": [
        "Note that ``Tacotron2.infer`` method perfoms multinomial sampling,\n",
        "therefore, the process of generating the spectrogram incurs randomness.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xvxKR1mHlEx-"
      },
      "outputs": [],
      "source": [
        "def plot():\n",
        "    fig, ax = plt.subplots(3, 1)\n",
        "    for i in range(3):\n",
        "        with torch.inference_mode():\n",
        "            spec, spec_lengths, _ = tacotron2.infer(processed, lengths)\n",
        "        print(spec[0].shape)\n",
        "        ax[i].imshow(spec[0].cpu().detach(), origin=\"lower\", aspect=\"auto\")\n",
        "\n",
        "\n",
        "plot()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M5k8NTJwlEx_"
      },
      "source": [
        "## Waveform Generation\n",
        "\n",
        "Once the spectrogram is generated, the last process is to recover the\n",
        "waveform from the spectrogram using a vocoder.\n",
        "\n",
        "``torchaudio`` provides vocoders based on ``GriffinLim`` and\n",
        "``WaveRNN``.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mtYx-mNBlEx_"
      },
      "source": [
        "### WaveRNN Vocoder\n",
        "\n",
        "Continuing from the previous section, we can instantiate the matching\n",
        "WaveRNN model from the same bundle.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "052VAwMRlEyA"
      },
      "outputs": [],
      "source": [
        "bundle = torchaudio.pipelines.TACOTRON2_WAVERNN_PHONE_LJSPEECH\n",
        "\n",
        "processor = bundle.get_text_processor()\n",
        "tacotron2 = bundle.get_tacotron2().to(device)\n",
        "vocoder = bundle.get_vocoder().to(device)\n",
        "\n",
        "text = \"Hello world! Text to speech!\"\n",
        "\n",
        "with torch.inference_mode():\n",
        "    processed, lengths = processor(text)\n",
        "    processed = processed.to(device)\n",
        "    lengths = lengths.to(device)\n",
        "    spec, spec_lengths, _ = tacotron2.infer(processed, lengths)\n",
        "    waveforms, lengths = vocoder(spec, spec_lengths)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eKAQWcl9lEyA"
      },
      "outputs": [],
      "source": [
        "def plot(waveforms, spec, sample_rate):\n",
        "    waveforms = waveforms.cpu().detach()\n",
        "\n",
        "    fig, [ax1, ax2] = plt.subplots(2, 1)\n",
        "    ax1.plot(waveforms[0])\n",
        "    ax1.set_xlim(0, waveforms.size(-1))\n",
        "    ax1.grid(True)\n",
        "    ax2.imshow(spec[0].cpu().detach(), origin=\"lower\", aspect=\"auto\")\n",
        "    return IPython.display.Audio(waveforms[0:1], rate=sample_rate)\n",
        "\n",
        "\n",
        "plot(waveforms, spec, vocoder.sample_rate)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d50bDCAjlEyB"
      },
      "source": [
        "### Griffin-Lim Vocoder\n",
        "\n",
        "Using the Griffin-Lim vocoder is same as WaveRNN. You can instantiate\n",
        "the vocoder object with\n",
        ":py:func:`~torchaudio.pipelines.Tacotron2TTSBundle.get_vocoder`\n",
        "method and pass the spectrogram.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y7jNLYb4lEyB"
      },
      "outputs": [],
      "source": [
        "bundle = torchaudio.pipelines.TACOTRON2_GRIFFINLIM_PHONE_LJSPEECH\n",
        "\n",
        "processor = bundle.get_text_processor()\n",
        "tacotron2 = bundle.get_tacotron2().to(device)\n",
        "vocoder = bundle.get_vocoder().to(device)\n",
        "\n",
        "with torch.inference_mode():\n",
        "    processed, lengths = processor(text)\n",
        "    processed = processed.to(device)\n",
        "    lengths = lengths.to(device)\n",
        "    spec, spec_lengths, _ = tacotron2.infer(processed, lengths)\n",
        "waveforms, lengths = vocoder(spec, spec_lengths)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Iznw-LY5lEyC"
      },
      "outputs": [],
      "source": [
        "plot(waveforms, spec, vocoder.sample_rate)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cWWaO8BalEyC"
      },
      "source": [
        "### Waveglow Vocoder\n",
        "\n",
        "Waveglow is a vocoder published by Nvidia. The pretrained weights are\n",
        "published on Torch Hub. One can instantiate the model using ``torch.hub``\n",
        "module.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "31VBW6rzlEyD"
      },
      "outputs": [],
      "source": [
        "# Workaround to load model mapped on GPU\n",
        "# https://stackoverflow.com/a/61840832\n",
        "waveglow = torch.hub.load(\n",
        "    \"NVIDIA/DeepLearningExamples:torchhub\",\n",
        "    \"nvidia_waveglow\",\n",
        "    model_math=\"fp32\",\n",
        "    pretrained=False,\n",
        ")\n",
        "checkpoint = torch.hub.load_state_dict_from_url(\n",
        "    \"https://api.ngc.nvidia.com/v2/models/nvidia/waveglowpyt_fp32/versions/1/files/nvidia_waveglowpyt_fp32_20190306.pth\",  # noqa: E501\n",
        "    progress=False,\n",
        "    map_location=device,\n",
        ")\n",
        "state_dict = {key.replace(\"module.\", \"\"): value for key, value in checkpoint[\"state_dict\"].items()}\n",
        "\n",
        "waveglow.load_state_dict(state_dict)\n",
        "waveglow = waveglow.remove_weightnorm(waveglow)\n",
        "waveglow = waveglow.to(device)\n",
        "waveglow.eval()\n",
        "\n",
        "with torch.no_grad():\n",
        "    waveforms = waveglow.infer(spec)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "92S6xXxUlEyD"
      },
      "outputs": [],
      "source": [
        "plot(waveforms, spec, 22050)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.15"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}